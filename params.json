{"name":"Phantom","tagline":"Asynchronous type-safe Scala DSL for Cassandra","body":"phantom [![Build Status](https://travis-ci.org/newzly/phantom.png?branch=develop)](https://travis-ci.org/newzly/phantom)\r\n\r\n==============\r\nAsynchronous Scala DSL for Cassandra\r\n\r\n\r\nUsing phantom\r\n=============\r\n\r\nThe current version is: ```val phantomVersion = 0.7.0```.\r\nPhantom is published to Maven Central and it's actively and avidly developed.\r\n\r\n<a id=\"table-of-contents\">Table of contents</a>\r\n===============================================\r\n\r\n<ol>\r\n    <li><a href=\"#issues-and-questions\">Issues and questions</a></li>\r\n    <li><a href=\"#integrating-phantom\">Integrating phantom in your project</a></li>\r\n    <li>\r\n        <p>phantom columns</p>\r\n        <ul>\r\n            <li><a href=\"#primitive-columns\">Primitive columns</a></li>\r\n            <li><a href=\"#optional-primitive-columns\">Optional primitive columns</a></li>\r\n            <li><a href=\"#collection-columns\">Collection columns</a></li>\r\n            <li>\r\n                <p><a href=\"#indexing-columns\">Indexing columns</a></p>\r\n                <ul>\r\n                    <li><a href=\"#partition-key\">Partition Key</a></li>\r\n                    <li><a href=\"#primary-key\">Primary Key</a></li>\r\n                    <li><a href=\"#secondary-key\">Secondary Index Key</a></li>\r\n                    <li><a href=\"#clustering-order\">Clustering Order Key</a></li>\r\n                </ul>\r\n            </li>\r\n            <li><a href=\"#thrift-columns\">Thrift columns</a></li>\r\n        </ul>\r\n    </li>\r\n    <li><a href=\"#data-modeling\">Data modeling with phantom</a></li>\r\n    <li>\r\n        <p><a href=\"#querying-with-phantom\">Querying with phantom</a></p>\r\n        <ul>\r\n            <li><a href=\"#select-queries\">SELECT queries</a></li>\r\n            <li><a href=\"#partial-select-queries\">Partial SELECT queries</a></li>\r\n            <li><a href=\"#where-and-operators\">WHERE and AND clause operators</a></li>\r\n            <li><a href=\"#insert-queries\">INSERT queries</a></li>\r\n            <li><a href=\"#update-queries\">UPDATE queries</a></li>\r\n            <li><a href=\"#delete-queries\">DELETE queries</a></li>\r\n            <li><a href=\"#truncate-queries\">TRUNCATE queries</a></li>\r\n            <li><a href=\"#create-queries\">CREATE queries</a></li>\r\n            <li><a href=\"#alter-queries\">ALTER queries</a></li>\r\n        </ul>\r\n    </li>\r\n    <li>\r\n        <p>Basic query examples</p>\r\n        <ul>\r\n            <li><a href=\"#scala-futures\">Using Scala Futures to query</a></li>\r\n            <li><a href=\"#scala-futures-examples\">Examples with Scala Futures</a></li>\r\n            <li><a href=\"#twitter-futures\">Using Twitter Futures to query</a></li>\r\n            <li><a href=\"#twitter-futures-examples\">Examples with Twitter Futures</a></li>\r\n        </ul>\r\n    </li>\r\n    <li>\r\n        <p><a href=\"#collections-and-operators\">Collection operators</a></p>\r\n        <ul>\r\n            <li><a href=\"#list-operators\">List operators</a></li>\r\n            <li><a href=\"#set-operators\">Set operators</a></li>\r\n            <li><a href=\"#map-operators\">Map operators</a></li>\r\n        </ul>\r\n    </li>\r\n    <li><a href=\"#automated-schema-generation\">Automated schema generation</a></li>\r\n    <li>\r\n        <p>Cassandra indexing</p>\r\n        <ul>\r\n            <li><a href=\"#partition-tokens\">Using partition tokens</a></li>\r\n            <li><a href=\"#partition-token-operators\">Partition token operators</a></li>\r\n            <li><a href=\"#compound-keys\">Compound Keys</a></li>\r\n            <li><a href=\"#composite-keys\">Composite Keys</a></li>\r\n            <li><a href=\"#time-series\">Cassandra Time Series and ClusteringOrder</a></li>\r\n            <li><a href=\"#secondary-keys\">Secondary Keys</a></li>\r\n        </ul>\r\n    </li>\r\n    <li><a href=\"#async-iterators\">Asynchronous iterators</a></li>\r\n    <li>\r\n        <p>Batch statements</p>\r\n        <ul>\r\n            <li><a href=\"#logged-batch-statements\">LOGGED Batch statements</a></li>\r\n            <li><a href=\"#counter-batch-statements\">COUNTER Batch statements</li>\r\n            <li><a href=\"#<a id=\"logged-batch-statements\">UNLOGGED Batch statements</a></li>\r\n        </ul>\r\n    </li>\r\n    <li><a href=\"#thrift-integration\">Thrift integration</a></li>\r\n    <li><a href=\"#running-tests\">Running the tests locally</a></li>\r\n    <li>\r\n        <p><a href=\"#contributors\">Contributing to phantom</a></p>\r\n        <ul>\r\n            <li><a href=\"#using-gitflow\">Using GitFlow as a branching model</a></li>\r\n            <li><a href=\"#scala-style-guidelines\">Scala style guidelines for contributions</a></li>\r\n            <li>Using the testing utilities to write tests</li>\r\n        </ul>\r\n    <li><a href=\"#copyright\">Copyright</a></li>\r\n</ol>\r\n\r\n\r\n<a id=\"issues-and-questions\">Issues and questions</a>\r\n=====================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nWe love Cassandra to bits and use it in every bit our stack. phantom makes it super trivial for Scala users to embrace Cassandra.\r\n\r\nCassandra is highly scalable and it's by far the most powerful database technology available, open source or otherwise.\r\n\r\nPhantom is built on top of the [Datastax Java Driver](https://github.com/datastax/java-driver), which does most of the heavy lifting. \r\n\r\nIf you're completely new to Cassandra, a much better place to start is the [Datastax Introduction to Cassandra](http://www.datastax.com/documentation/getting_started/doc/getting_started/gettingStartedIntro_r.html)\r\n\r\nWe are very happy to help implement missing features in phantom, answer questions about phantom, and occasionally help you out with Cassandra questions, although do note we're a bit short staffed!\r\n\r\nYou can get in touch via the [newzly-phantom](https://groups.google.com/forum/#!forum/newzly-phantom) Google Group.\r\n\r\n<a id=\"integrating-phantom\">Integrating phantom in your project</a>\r\n===================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nFor most things, all you need is ```phantom-dsl```. Read through for information on other modules.\r\n\r\n```scala\r\nlibraryDependencies ++= Seq(\r\n  \"com.newzly\"  %% \"phantom-dsl\"                   % phantomVersion\r\n)\r\n```\r\n\r\nThe full list of available modules is:\r\n\r\n```scala\r\nlibraryDependencies ++= Seq(\r\n  \"com.newzly\"  %% \"phantom-dsl\"                   % phantomVersion,\r\n  \"com.newzly\"  %% \"phantom-cassandra-unit\"        % phantomVersion,\r\n  \"com.newzly\"  %% \"phantom-example\"               % phantomVersion,\r\n  \"com.newzly\"  %% \"phantom-thrift\"                % phantomVersion,\r\n  \"com.newzly\"  %% \"phantom-test\"                  % phantomVersion\r\n)\r\n```\r\n\r\n<a id=\"primitive-columns\">Primitive columns</a>\r\n====================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThis is the list of available columns and how they map to C* data types.\r\nThis also includes the newly introduced ```static``` columns in C* 2.0.6.\r\n\r\nThe type of a static column can be any of the allowed primitive Cassandra types.\r\nphantom won't let you mixin a non-primitive via implicit magic.\r\n\r\n| phantom columns               | Java/Scala type           | Cassandra type    |\r\n| ---------------               |-------------------        | ----------------- |\r\n| BigDecimalColumn              | scala.math.BigDecimal     | decimal           |\r\n| BigIntColumn                  | scala.math.BigInt         | varint            |\r\n| BooleanColumn                 | scala.Boolean             | boolean           |\r\n| DateColumn                    | java.util.Date            | timestamp         |\r\n| DateTimeColumn                | org.joda.time.DateTime    | timestamp         |\r\n| DoubleColumn                  | scala.Double              | double            |\r\n| FloatColumn                   | scala.Float               | float             |\r\n| IntColumn                     | scala.Int                 | int               |\r\n| InetAddressColumn             | java.net.InetAddress      | inet              |\r\n| LongColumn                    | scala.Long                | long              |\r\n| StringColumn                  | java.lang.String          | text              |\r\n| UUIDColumn                    | java.util.UUID            | uuid              |\r\n| TimeUUIDColumn                | java.util.UUID            | timeuuid          |\r\n| CounterColumn                 | scala.Long                | counter           |\r\n| StaticColumn&lt;type&gt;      | &lt;type&gt;              | type static       |\r\n\r\n\r\n<a id=\"optional-primitive-columns\">Optional primitive columns</a>\r\n===================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nOptional columns allow you to set a column to a ```null``` or a ```None```. Use them when you really want something to be optional.\r\nThe outcome is that instead of a ```T``` you get an ```Option[T]``` and you can ```match, fold, flatMap, map``` on a ```None```.\r\n\r\nThe ```Optional``` part is handled at a DSL level, it's not translated to Cassandra in any way.\r\n\r\n| phantom columns               | Java/Scala type                   | Cassandra columns |\r\n| ---------------               | -------------------------         | ----------------- |\r\n| OptionalBigDecimalColumn      | Option[scala.math.BigDecimal]     | decimal           |\r\n| OptionalBigIntColumn          | Option[scala.math.BigInt]         | varint            |\r\n| OptionalBooleanColumn         | Option[scala.Boolean]             | boolean           |\r\n| OptionalDateColumn            | Option[java.util.Date]            | timestamp         |\r\n| OptionalDateTimeColumn        | Option[org.joda.time.DateTime]    | timestamp         |\r\n| OptionalDoubleColumn          | Option[scala.Double]              | double            |\r\n| OptionalFloatColumn           | Option[scala.Float]               | float             |\r\n| OptionalIntColumn             | Option[scala.Int]                 | int               |\r\n| OptionalInetAddressColumn     | Option[java.net.InetAddress]      | inet              |\r\n| OptionalLongColumn            | Option[Long]                      | long              |\r\n| OptionalStringColumn          | Option[java.lang.String]          | text              |\r\n| OptionalUUIDColumn            | Option[java.util.UUID]            | uuid              |\r\n| OptionalTimeUUID              | Option[java.util.UUID]            | timeuuid          |\r\n\r\n\r\n<a id=\"collection-columns\">Collection columns</a>\r\n======================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nCassandra collections do not allow custom data types. Storing JSON as a string is possible, but it's still a ```text``` column as far as Cassandra is concerned.\r\nThe ```type``` in the below example is always a default C* type.\r\n\r\n| phantom columns                     | Cassandra columns       |\r\n| ---------------                     | -----------------       |\r\n| ListColumn.&lt;type&gt;             | list&lt;type&gt;        |\r\n| SetColumn.&lt;type&gt;              | set&lt;type&gt;         |\r\n| MapColumn.&lt;type, type&gt;        | map&lt;type, type&gt;   |\r\n\r\n<a id=\"indexing-columns\">Indexing columns</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nphantom uses a specific set of traits to enforce more advanced Cassandra limitations and schema rules at compile time.\r\n\r\n<a id=\"partition-key\">PartitionKey[T]</a>\r\n==============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThis is the default partitioning key of the table, telling Cassandra how to divide data into partitions and store them accordingly.\r\nYou must define at least one partition key for a table. Phantom will gently remind you of this with a fatal error.\r\n\r\nIf you use a single partition key, the ```PartitionKey``` will always be the first ```PrimaryKey``` in the schema.\r\n\r\nIt looks like this in CQL: ```PRIMARY_KEY(your_partition_key, primary_key_1, primary_key_2)```.\r\n\r\nUsing more than one ```PartitionKey[T]``` in your schema definition will output a Composite Key in Cassandra.\r\n```PRIMARY_KEY((your_partition_key_1, your_partition_key2), primary_key_1, primary_key_2)```.\r\n\r\n\r\n<a id=\"primary-key\">PrimaryKey[T]</a>\r\n==============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nAs it's name says, using this will mark a column as ```PrimaryKey```. Using multiple values will result in a Compound Value.\r\nThe first ```PrimaryKey``` is used to partition data. phantom will force you to always define a ```PartitionKey``` so you don't forget\r\nabout how your data is partitioned. We also use this DSL restriction because we hope to do more clever things with it in the future.\r\n\r\nA compound key in C* looks like this:\r\n```PRIMARY_KEY(primary_key, primary_key_1, primary_key_2)```.\r\n\r\nBefore you add too many of these, remember they all have to go into a ```where``` clause.\r\nYou can only query with a full primary key, even if it's compound. phantom can't yet give you a compile time error for this, but Cassandra will give you a runtime one.\r\n\r\n<a id=\"secondary-key\">Index</a>\r\n==============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThis is a SecondaryIndex in Cassandra. It can help you enable querying really fast, but it's not exactly high performance.\r\nIt's generally best to avoid it, we implemented it to show off what good guys we are.\r\n\r\nWhen you mix in ```Index[T]``` on a column, phantom will let you use it in a ```where``` clause.\r\nHowever, don't forget to ```allowFiltering``` for such queries, otherwise C* will give you an error.\r\n\r\n<a id=\"clustering-order\">ClusteringOrder</a>\r\n=================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThis can be used with either ```java.util.Date``` or ```org.joda.time.DateTime```. It tells Cassandra to store records in a certain order based on this field.\r\n\r\nAn example might be: ```object timestamp extends DateTimeColumn(this) with ClusteringOrder[DateTime] with Ascending```\r\nTo fully define a clustering column, you MUST also mixin either ```Ascending``` or ```Descending``` to indicate the sorting order.\r\n\r\n\r\n\r\n<a id=\"thrift-columns\">Thrift Columns</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThese columns are especially useful if you are building Thrift services. They are deeply integrated with Twitter Scrooge and relevant to the Twitter ecosystem(Finagle, Zipkin, Storm etc)\r\nThey are available via the ```phantom-thrift``` module and you need to ```import com.newzly.phantom.thrift.Implicits._``` to get them.\r\n\r\nIn the below scenario, the C* type is always text and the type you need to pass to the column is a Thrift struct, specifically ```com.twitter.scrooge.ThriftStruct```.\r\nphantom will use a ```CompactThriftSerializer```, store the record as a binary string and then reparse it on fetch.\r\n\r\nThrift serialization and de-serialization is extremely fast, so you don't need to worry about speed or performance overhead.\r\nYou generally use these to store collections(small number of items), not big things.\r\n\r\n| phantom columns                     | Cassandra columns       |\r\n| ---------------                     | -----------------       |\r\n| ThriftColumn.&lt;type&gt;           | text                    |\r\n| ThriftListColumn.&lt;type&gt;       | list&lt;text&gt;        |\r\n| ThriftSetColumn.&lt;type&gt;        | set&lt;text&gt;         |\r\n| ThriftMapColumn.&lt;type, type&gt;  | map&lt;text, text&gt;   |\r\n\r\n\r\n\r\n<a id=\"data-modeling\">Data modeling with phantom</a>\r\n====================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport java.util.{ Date, UUID }\r\nimport com.datastax.driver.core.Row\r\nimport com.newzly.phantom.sample.ExampleModel\r\nimport com.newzly.phantom.Implicits._\r\n\r\ncase class ExampleModel (\r\n  id: Int,\r\n  name: String,\r\n  props: Map[String, String],\r\n  timestamp: Int,\r\n  test: Option[Int]\r\n)\r\n\r\nsealed class ExampleRecord extends CassandraTable[ExampleRecord, ExampleModel] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object timestamp extends DateTimeColumn(this) with ClusteringOrder with Ascending\r\n  object name extends StringColumn(this)\r\n  object props extends MapColumn[ExampleRecord, ExampleModel, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n\r\n```\r\n\r\n<a id=\"querying-with-phantom\">Querying with Phantom</a>\r\n=======================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThe query syntax is inspired by the Foursquare Rogue library and aims to replicate CQL 3 as much as possible.\r\n\r\nPhantom works with both Scala Futures and Twitter Futures as first class citizens.\r\n\r\n\r\n<a id=\"select-queries\">\"Select\" queries</a>\r\n================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Method name                       | Description                                                                           |\r\n| --------------------------------- | ------------------------------------------------------------------------------------- |\r\n| ```where```                       | The ```WHERE``` clause in CQL                                                         |\r\n| ```and```                         | Chains several clauses, creating a ```WHERE ... AND``` query                          |\r\n| ```orderBy```                     | Adds an ```ORDER_BY column_name``` to the query                                       |\r\n| ```allowFiltering```              | Allows Cassandra to filter records in memory. This is an expensive operation.         |\r\n| ```useConsistencyLevel```         | Sets the consistency level to use.                                                    |\r\n| ```setFetchSize       ```         | Sets the maximum number of records to retrieve. Default is 10000                      |\r\n| ```limit```                       | Sets the exact number of records to retrieve.                                         |\r\n\r\n\r\nSelect queries are very straightforward and enforce most limitations at compile time.\r\n\r\n\r\n<a id=\"where-and-operators\">```where``` and ```and``` clause operators</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Operator name      | Description                                                              |\r\n| ------------------ | ------------------------------------------------------------                                             |\r\n| eqs                | The \"equals\" operator. Will match if the objects are equal                                               |\r\n| in                 | The \"in\" operator. Will match if the object is found the list of arguments                               |\r\n| gt                 | The \"greater than\" operator. Will match a the record is greater than the argument and exists             |\r\n| gte                | The \"greater than or equals\" operator. Will match a the record is greater than the argument and exists   |\r\n| lt                 | The \"lower than\" operator. Will match a the record that is less than the argument and exists             |\r\n| lte                | The \"lower than or equals\" operator. Will match a the record that is less than the argument and exists   |\r\n\r\n\r\n<a id=\"partial-select-queries\">Partial selects</a>\r\n===================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nAll partial select queries will return Tuples and are therefore limited to 22 fields.\r\nWe haven't yet bothered to add more than 10 fields in the select, but you can always do a Pull Request.\r\nThe file you are looking for is [here](https://github.com/newzly/phantom/blob/develop/phantom-dsl/src/main/scala/com/newzly/phantom/SelectTable.scala).\r\nThe 22 field limitation will change in Scala 2.11 and phantom will be updated once cross version compilation is enabled.\r\n\r\n```scala\r\n  def getNameById(id: UUID): Future[Option[String]] = {\r\n    ExampleRecord.select(_.name).where(_.id eqs someId).one()\r\n  }\r\n\r\n  def getNameAndPropsById(id: UUID): Future[Option(String, Map[String, String])] {\r\n    ExampleRecord.select(_.name, _.props).where(_.id eqs someId).one()\r\n  }\r\n```\r\n\r\n<a id=\"insert-queries\">\"Insert\" queries</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Method name                       | Description                                                                           |\r\n| --------------------------------- | ------------------------------------------------------------------------------------- |\r\n| ```value```                       | A type safe Insert query builder. Throws an error for ```null``` values.              |\r\n| ```valueOrNull```                 | This will accept a ```null``` without throwing an error.                              |\r\n| ```useConsistencyLevel```         | Sets the consistency level to use.                                                    |\r\n| ```ttl```                         | Sets the \"Time-To-Live\" for the record.                                               |\r\n\r\n\r\n<a id=\"update-queries\">\"Update\" queries</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Method name                       | Description                                                                           |\r\n| --------------------------------- | ------------------------------------------------------------------------------------- |\r\n| ```where```                       | The ```WHERE``` clause in CQL                                                         |\r\n| ```and```                         | Chains several clauses, creating a ```WHERE ... AND``` query                          |\r\n| ```modify```                      | The actual update query builder                                                       |\r\n| ```useConsistencyLevel```         | Sets the consistency level to use.                                                    |\r\n| ```onflyIf```                     | Addition update condition. Used on non-primary columns                                |\r\n\r\n\r\n<a id=\"delete-queries\">\"Delete\" queries</a>\r\n===========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Method name                       | Description                                                                           |\r\n| --------------------------------- | ------------------------------------------------------------------------------------- |\r\n| ```where```                       | The ```WHERE``` clause in CQL                                                         |\r\n| ```useConsistencyLevel```         | Sets the consistency level to use.                                                    |\r\n\r\n\r\n<a id=\"scala-futures\">Scala Futures</a>\r\n=======================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\nExampleRecord.select.one() // When you only want to select one record\r\nExampleRecord.update.where(_.name eqs name).modify(_.name setTo \"someOtherName\").future() // When you don't care about the return type.\r\nExampleRecord.select.fetchEnumerator // when you need an Enumerator\r\nExampleRecord.select.fetch // When you want to fetch a Seq[Record]\r\n```\r\n\r\n<a id=\"scala-futures-examples\">Examples with Scala Futures</a>\r\n================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n\r\n```scala\r\n\r\nimport scala.concurrent.ExecutionContext.Implicits.global\r\nimport scala.concurrent.Future\r\n\r\nobject ExampleRecord extends ExampleRecord {\r\n  override val tableName = \"examplerecord\"\r\n\r\n  // now define a session, a normal Datastax cluster connection\r\n  implicit val session = SomeCassandraClient.session;\r\n\r\n  def getRecordsByName(name: String): Future[Seq[ExampleModel]] = {\r\n    ExampleRecord.select.where(_.name eqs name).fetch\r\n  }\r\n\r\n  def getOneRecordByName(name: String, someId: UUID): Future[Option[ExampleModel]] = {\r\n    ExampleRecord.select.where(_.name eqs name).and(_.id eqs someId).one()\r\n  }\r\n}\r\n```\r\n\r\n<a id=\"twitter-futures\">Twitter Futures</a>\r\n===========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\nExampleRecord.select.get() // When you only want to select one record\r\nExampleRecord.update.where(_.name eqs name).modify(_.name setTo \"someOtherName\").execute() // When you don't care about the return type.\r\nExampleRecord.select.enumerate // when you need an Enumerator\r\nExampleRecord.select.collect // When you want to fetch a Seq[Record]\r\n```\r\n\r\n<a id=\"twitter-futures-examples\">More examples with Twitter Futures</a>\r\n=======================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport com.twitter.util.Future\r\n\r\nobject ExampleRecord extends ExampleRecord {\r\n  override val tableName = \"examplerecord\"\r\n\r\n  // now define a session, a normal Datastax cluster connection\r\n  implicit val session = SomeCassandraClient.session;\r\n\r\n  def getRecordsByName(name: String): Future[Seq[ExampleModel]] = {\r\n    ExampleRecord.select.where(_.name eqs name).collect\r\n  }\r\n\r\n  def getOneRecordByName(name: String, someId: UUID): Future[Option[ExampleModel]] = {\r\n    ExampleRecord.select.where(_.name eqs name).and(_.id eqs someId).get()\r\n  }\r\n}\r\n```\r\n\r\n<a id=\"collections-and-operators\">Collections and operators</a>\r\n================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nBased on the above list of columns, phantom supports CQL 3 modify operations for CQL 3 collections: ```list, set, map```.\r\nAll operators will be available in an update query, specifically:\r\n\r\n```ExampleRecord.update.where(_.id eqs someId).modify(_.someList $OPERATOR $args).future()```.\r\n\r\n<a id=\"list-operators\">List operators</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nExamples in [ListOperatorsTest.scala](https://github.com/newzly/phantom/blob/develop/phantom-test/src/test/scala/com/newzly/phantom/dsl/crud/ListOperatorsTest.scala).\r\n\r\n| Name                          | Description                                   |\r\n| ----------------------------- | --------------------------------------------- |\r\n| ```prepend```                 | Adds an item to the head of the list          |\r\n| ```prependAll```              | Adds multiple items to the head of the list   |\r\n| ```append```                  | Adds an item to the tail of the list          |\r\n| ```appendAll```               | Adds multiple items to the tail of the list   |\r\n| ```discard```                 | Removes the given item from the list.         |\r\n| ```discardAll```              | Removes all given items from the list.        |\r\n| ```setIdIx```                 | Updates a specific index in the list          |\r\n\r\n<a id=\"set-operators\">Set operators</a>\r\n=======================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nSets have a better performance than lists, as the Cassandra documentation suggests.\r\nExamples in [SetOperationsTest.scala](https://github.com/newzly/phantom/blob/develop/phantom-test/src/test/scala/com/newzly/phantom/dsl/crud/SetOperationsTest.scala).\r\n\r\n| Name                          | Description                                   |\r\n| ----------------------------- | --------------------------------------------- |\r\n| ```add```                     | Adds an item to the tail of the set           |\r\n| ```addAll```                  | Adds multiple items to the tail of the set    |\r\n| ```remove ```                 | Removes the given item from the set.          |\r\n| ```removeAll```               | Removes all given items from the set.         |\r\n\r\n\r\n<a id=\"map-operators\">Map operators</a>\r\n=======================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nBoth the key and value types of a Map must be Cassandra primitives.\r\nExamples in [MapOperationsTest.scala](https://github.com/newzly/phantom/blob/develop/phantom-test/src/test/scala/com/newzly/phantom/dsl/crud/MapOperationsTest.scala):\r\n\r\n| Name                          | Description                                   |\r\n| ----------------------------- | --------------------------------------------- |\r\n| ```put```                     | Adds an (key -> value) pair to the map        |\r\n| ```putAll```                  | Adds multiple (key -> value) pairs to the map |\r\n\r\n\r\n<a id=\"automated-schema-generation\">Automated schema generation</a>\r\n===================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nReplication strategies and more advanced features are not yet available in phantom, but CQL 3 Table schemas are  automatically generated from the Scala code. To create a schema in Cassandra from a table definition:\r\n\r\n```scala\r\n\r\nimport scala.concurrent.Await\r\nimport scala.concurrent.duration._\r\n\r\nAwait.result(ExampleRecord.create().future(), 5000 millis)\r\n```\r\n\r\nOf course, you don't have to block unless you want to.\r\n\r\n\r\n<a id=\"partition-tokens\">Partition tokens</a>\r\n==============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport scala.concurrent.Await\r\nimport scala.concurrent.duration._\r\nimport com.newzly.phantom.Implicits._\r\n\r\nsealed class ExampleRecord2 extends CassandraTable[ExampleRecord2, ExampleModel] with LongOrderKey[ExampleRecod2, ExampleRecord] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object timestamp extends DateTimeColumn(this)\r\n  object name extends StringColumn(this)\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n\r\n\r\nval orderedResult = Await.result(Articles.select.where(_.id gtToken one.get.id ).fetch, 5000 millis)\r\n\r\n```\r\n\r\n<a id=\"partition-token-operators\">PartitionToken operators</a>\r\n===============================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Operator name      | Description                                                              |\r\n| ------------------ | ------------------------------------------------------------                                             |\r\n| eqsToken           | The \"equals\" operator. Will match if the objects are equal                                               |\r\n| gtToken            | The \"greater than\" operator. Will match a the record is greater than the argument                        |\r\n| gteToken           | The \"greater than or equals\" operator. Will match a the record is greater than the argument              |\r\n| ltToken            | The \"lower than\" operator. Will match a the record that is less than the argument and exists             |\r\n| lteToken           | The \"lower than or equals\" operator. Will match a the record that is less than the argument              |\r\n\r\nFor more details on how to use Cassandra partition tokens, see [SkipRecordsByToken.scala]( https://github.com/newzly/phantom/blob/develop/phantom-test/src/test/scala/com/newzly/phantom/dsl/SkipRecordsByToken.scala)\r\n\r\n\r\n<a id=\"time-series\">Cassandra Time Series</a>\r\n=============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nphantom supports Cassandra Time Series with both ```java.util.Date``` and ```org.joda.time.DateTime ```. To use them, simply mixin ```com.newzly.phantom.keys.ClusteringOrder``` and either ```Ascending``` or ```Descending```.\r\n\r\nRestrictions are enforced at compile time.\r\n\r\n```scala\r\n\r\nimport com.newzly.phantom.Implicits._\r\n\r\nsealed class ExampleRecord3 extends CassandraTable[ExampleRecord3, ExampleModel] with LongOrderKey[ExampleRecod3, ExampleRecord] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object timestamp extends DateTimeColumn(this) with ClusteringOrder with Ascending\r\n  object name extends StringColumn(this)\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n```\r\n\r\nAutomatic schema generation can do all the setup for you.\r\n\r\n\r\n<a id=\"compound-keys\">Compound keys</a>\r\n=======================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nPhantom also supports using Compound keys out of the box. The schema can once again by auto-generated.\r\n\r\nA table can have only one ```PartitionKey``` but several ```PrimaryKey``` definitions. Phantom will use these keys to build a compound value. Example scenario, with the compound key: ```(id, timestamp, name)```\r\n\r\n```scala\r\n\r\nimport org.joda.time.DateTime\r\nimport com.newzly.phantom.Implicits._\r\n\r\nsealed class ExampleRecord3 extends CassandraTable[ExampleRecord3, ExampleModel] with LongOrderKey[ExampleRecod3, ExampleRecord] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object timestamp extends DateTimeColumn(this) with PrimaryKey[DateTime]\r\n  object name extends StringColumn(this) with PrimaryKey[String]\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n```\r\n\r\n<a id=\"secondary-keys\">CQL 3 Secondary Keys</a>\r\n===============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nWhen you want to use a column in a ```where``` clause, you need an index on it. Cassandra data modeling is out of the scope of this writing, but phantom offers ```com.newzly.phantom.keys.Index``` to enable querying.\r\n\r\nThe CQL 3 schema for secondary indexes can also be auto-generated with ```ExampleRecord4.create()```.\r\n\r\n```SELECT``` is the only query you can perform with an ```Index``` column. This is a Cassandra limitation. The relevant tests are found [here](https://github.com/newzly/phantom/blob/develop/phantom-test/src/test/scala/com/newzly/phantom/dsl/specialized/SecondaryIndexTest.scala).\r\n\r\n\r\n```scala\r\n\r\nimport java.util.UUID\r\nimport org.joda.time.DateTime\r\nimport com.newzly.phantom.Implicits._\r\n\r\nsealed class ExampleRecord4 extends CassandraTable[ExampleRecord4, ExampleModel] with LongOrderKey[ExampleRecod4, ExampleRecord] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object timestamp extends DateTimeColumn(this) with Index[DateTime]\r\n  object name extends StringColumn(this) with Index[String]\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n```\r\n\r\n<a id=\"async-iterators\">Asynchronous iterators for large record sets</a>\r\n========================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nPhantom comes packed with CQL rows asynchronous lazy iterators to help you deal with billions of records.\r\nphantom iterators are based on Play iterators with very lightweight integration.\r\n\r\nThe functionality is identical with respect to asynchronous, lazy behaviour and available methods.\r\nFor more on this, see this [Play tutorial](\r\nhttp://mandubian.com/2012/08/27/understanding-play2-iteratees-for-normal-humans/)\r\n\r\n\r\nUsage is trivial. If you want to use ```slice, take or drop``` with iterators, the partitioner needs to be ordered.\r\n\r\n```scala\r\n\r\nimport scala.concurrent.Await\r\nimport scala.concurrent.duration._\r\nimport org.joda.time.DateTime\r\nimport com.newzly.phantom.Implicits._\r\n\r\n\r\nsealed class ExampleRecord3 extends CassandraTable[ExampleRecord3, ExampleModel] with LongOrderKey[ExampleRecord3, ExampleRecord] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object timestamp extends DateTimeColumn(this) with PrimaryKey[DateTime]\r\n  object name extends StringColumn(this) with PrimaryKey[String]\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n\r\nobject ExampleRecord3 extends ExampleRecord3 {\r\n  def getRecords(start: Int, limit: Int): Future[Set[ExampleModel]] = {\r\n    select.fetchEnumerator.slice(start, limit).collect\r\n  }\r\n}\r\n\r\n```\r\n\r\n<a id=\"batch-statements\">Batch statements</a>\r\n=============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nphantom also brrings in support for batch statements. To use them, see [IterateeBigTest.scala]( https://github.com/newzly/phantom/blob/develop/phantom-test/src/test/scala/com/newzly/phantom/iteratee/IterateeBigTest.scala)\r\n\r\nWe have tested with 10,000 statements per batch, and 1000 batches processed simulatenously. Before you run the test, beware that it takes ~40 minutes.\r\n\r\nBatches use lazy iterators and daisy chain them to offer thread safe behaviour. They are not memory intensive and you can expect consistent processing speed even with 1 000 000 statements per batch.\r\n\r\nBatches are immutable and adding a new record will result in a new Batch, just like most things Scala, so be careful to chain the calls.\r\n\r\nphantom also supports COUNTER batch updates and UNLOGGED batch updates.\r\n\r\n\r\n<a id=\"logged-batch-statements\">LOGGED batch statements</a>\r\n===========================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport com.newzly.phantom.Implicits._\r\n\r\nBatchStatement()\r\n    .add(ExampleRecord.update.where(_.id eqs someId).modify(_.name setTo \"blabla\"))\r\n    .add(ExampleRecord.update.where(_.id eqs someOtherId).modify(_.name setTo \"blabla2\"))\r\n    .future()\r\n\r\n```\r\n\r\n<a id=\"counter-batch-statements\">COUNTER batch statements</a>\r\n============================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport com.newzly.phantom.Implicits._\r\n\r\nCounterBatchStatement()\r\n    .add(ExampleRecord.update.where(_.id eqs someId).modify(_.someCounter increment 500L))\r\n    .add(ExampleRecord.update.where(_.id eqs someOtherId).modify(_.someCounter decrement 300L))\r\n    .future()\r\n```\r\n\r\n<a id=\"unlogged-batch-statements\">UNLOGGED batch statements</a>\r\n============================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport com.newzly.phantom.Implicits._\r\n\r\nUnloggedBatchStatement()\r\n    .add(ExampleRecord.update.where(_.id eqs someId).modify(_.name setTo \"blabla\"))\r\n    .add(ExampleRecord.update.where(_.id eqs someOtherId).modify(_.name setTo \"blabla2\"))\r\n    .future()\r\n\r\n```\r\n\r\n<a id=\"thrift-integration\">Thrift integration</a>\r\n=================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nWe use Apache Thrift extensively for our backend services. ```phantom``` is very easy to integrate with Thrift models and uses ```Twitter Scrooge``` to compile them. Thrift integration is optional and available via ```\"com.newzly\" %% \"phantom-thrift\"  % phantomVersion```.\r\n\r\n```thrift\r\nnamespace java com.newzly.phantom.sample.ExampleModel\r\n\r\nstuct ExampleModel {\r\n  1: required i32 id,\r\n  2: required string name,\r\n  3: required Map&lt;string, string&gt; props,\r\n  4: required i32 timestamp\r\n  5: optional i32 test\r\n}\r\n```\r\n\r\n\r\n<a id=\"running-tests\">Running the tests locally</a>\r\n==================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nphantom uses Embedded Cassandra to run tests without a local Cassandra server running.\r\nYou need two terminals to run the tests, one for Embedded Cassandra and one for the actual tests.\r\n\r\n```scala\r\nsbt\r\nproject phantom-cassandra-unit\r\nrun\r\n```\r\n\r\nThen in a new terminal\r\n\r\n```scala\r\nsbt\r\nproject phantom-test\r\ntest\r\n```\r\n\r\n<a id=\"contributors\">Contributors</a>\r\n=====================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nPhantom was developed at newzly as an in-house project. All Cassandra integration at newzly goes through phantom.\r\n\r\n* Flavian Alexandru flavian@newzly.com(maintainer)\r\n* Tomasz Perek tomasz.perek@newzly.com\r\n* Viktor Taranenko (viktortnk)\r\n* Bartosz Jankiewicz (@bjankie1)\r\n* Eugene Zhulenev (@ezhulenev)\r\n\r\n<a id=\"copyright\">Copyright</a>\r\n===============================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nSpecial thanks to Viktor Taranenko from WhiskLabs, who gave us the original idea.\r\n\r\nCopyright 2013 WhiskLabs, Copyright 2013 - 2014 newzly.\r\n\r\n\r\nContributing to phantom\r\n=======================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nContributions are most welcome!\r\n\r\n<a id=\"git-flow\">Using GitFlow</a>\r\n==================================\r\n\r\nTo contribute, simply submit a \"Pull request\" via GitHub.\r\n\r\nWe use GitFlow as a branching model and SemVer for versioning.\r\n\r\n- When you submit a \"Pull request\" we require all changes to be squashed.\r\n- We never merge more than one commit at a time. All the n commits on your feature branch must be squashed.\r\n- We won't look at the pull request until Travis CI says the tests pass, make sure tests go well.\r\n\r\n<a id=\"style-guidelines\">Scala Style Guidelines</a>\r\n===================================================\r\n\r\nIn spirit, we follow the [Twitter Scala Style Guidelines](http://twitter.github.io/effectivescala/).\r\nWe will reject your pull request if it doesn't meet code standards, but we'll happily give you a hand to get it right.\r\n\r\nSome of the things that will make us seriously frown:\r\n\r\n- Blocking when you don't have to. It just makes our eyes hurt when we see useless blocking.\r\n- Testing should be thread safe and fully async, use ```ParallelTestExecution``` if you want to show off.\r\n- Use the common patterns you already see here, we've done a lot of work to make it easy.\r\n- Don't randomly import stuff. We are very big on alphabetized clean imports.\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}